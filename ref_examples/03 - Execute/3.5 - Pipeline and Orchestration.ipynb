{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "061768d6-a118-4a4d-8a70-f62be42a9695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 8px 16px; background: #F8F9FA; border-bottom: 2px solid #E0E0E0; margin: 0; line-height: 1;\">\n",
    "    <div style=\"font-size: 14px; color: #666;\">\n",
    "        <span style=\"font-weight: bold; color: #333;\">{SOURCE_PLATFORM} → Databricks Migration</span>\n",
    "        <span style=\"margin-left: 8px; color: #999;\">|</span>\n",
    "        <span style=\"margin-left: 8px;\">03 - Execute</span>\n",
    "    </div>\n",
    "    <div style=\"display: flex; align-items: center; gap: 8px;\">\n",
    "        <img src=\"https://cdn.simpleicons.org/snowflake/29B5E8\" width=\"24\" height=\"24\"/>\n",
    "        <span style=\"color: #999; font-size: 16px;\">→</span>\n",
    "        <img src=\"https://cdn.simpleicons.org/databricks/FF3621\" width=\"24\" height=\"24\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bcb015-2c66-4ea8-aa56-1dc700c37400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72d6ade-94be-4656-875f-0141150063a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline and Orchestration\n",
    "\n",
    "## Overview\n",
    "\n",
    "After migrating data and converting SQL, you must rebuild your data pipelines and scheduling in Databricks. This lesson covers Lakeflow Jobs, Spark Declarative Pipelines, and integration with external orchestrators for end-to-end workflow automation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Map {SOURCE_PLATFORM} scheduling concepts to Databricks orchestration\n",
    "- Build multi-task workflows with Lakeflow Jobs\n",
    "- Create declarative data pipelines with Spark Declarative Pipelines (DLT)\n",
    "- Integrate with external orchestrators (Airflow, Azure Data Factory)\n",
    "- Implement monitoring, alerting, and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "4303e19d-254e-4d8f-9998-31b905173a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Orchestration Options\n",
    "\n",
    "Databricks provides multiple orchestration tools depending on your needs.\n",
    "\n",
    "<br />\n",
    "<div class=\"mermaid\">\n",
    "flowchart TB\n",
    "    subgraph LAKEFLOW[\"Lakeflow\"]\n",
    "        JOBS[\"<b>Lakeflow Jobs</b><br/><i>Workflow orchestration</i>\"]\n",
    "        CONNECT[\"<b>Lakeflow Connect</b><br/><i>Managed ingestion</i>\"]\n",
    "    end\n",
    "    subgraph PIPELINES[\"Data Pipelines\"]\n",
    "        DLT[\"<b>Spark Declarative Pipelines</b><br/><i>Declarative ETL</i>\"]\n",
    "        SS[\"<b>Structured Streaming</b><br/><i>Real-time processing</i>\"]\n",
    "    end\n",
    "    subgraph EXTERNAL[\"External Orchestrators\"]\n",
    "        AIRFLOW[\"<b>Apache Airflow</b><br/><i>DAG-based orchestration</i>\"]\n",
    "        ADF[\"<b>Azure Data Factory</b><br/><i>Azure-native</i>\"]\n",
    "        OTHER[\"<b>Other</b><br/><i>Prefect, Dagster, etc.</i>\"]\n",
    "    end\n",
    "    JOBS --> DLT\n",
    "    JOBS --> SS\n",
    "    JOBS --> CONNECT\n",
    "    EXTERNAL --> JOBS\n",
    "    style LAKEFLOW fill:#e8f5e9,stroke:#4caf50\n",
    "    style PIPELINES fill:#e3f2fd,stroke:#1976d2\n",
    "    style EXTERNAL fill:#fff3e0,stroke:#ff9800\n",
    "</div>\n",
    "<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>\n",
    "\n",
    "### Tool Comparison\n",
    "\n",
    "| Tool | Best For | Complexity |\n",
    "|------|----------|------------|\n",
    "| **Lakeflow Jobs** | General workflow orchestration | Low |\n",
    "| **Spark Declarative Pipelines** | Declarative ETL with quality controls | Medium |\n",
    "| **Structured Streaming** | Real-time event processing | Medium-High |\n",
    "| **Apache Airflow** | Complex DAGs, cross-platform orchestration | High |\n",
    "| **Azure Data Factory** | Azure-native, hybrid workflows | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "10d22c9c-d13a-4e94-891a-2acb157da093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakeflow Jobs\n",
    "\n",
    "Lakeflow Jobs is the primary orchestration tool in Databricks, supporting multi-task workflows, dependencies, retries, and scheduling.\n",
    "\n",
    "<br />\n",
    "<div class=\"mermaid\">\n",
    "flowchart LR\n",
    "    subgraph JOB[\"Lakeflow Job\"]\n",
    "        T1[\"<b>Task 1</b><br/><i>Extract</i>\"]\n",
    "        T2[\"<b>Task 2</b><br/><i>Transform</i>\"]\n",
    "        T3[\"<b>Task 3</b><br/><i>Load</i>\"]\n",
    "        T4[\"<b>Task 4</b><br/><i>Validate</i>\"]\n",
    "    end\n",
    "    T1 --> T2 --> T3 --> T4\n",
    "    style JOB fill:#fff,stroke:#FF3621,stroke-width:2px\n",
    "    style T1 fill:#e3f2fd,stroke:#1976d2\n",
    "    style T2 fill:#fff3e0,stroke:#ff9800\n",
    "    style T3 fill:#e8f5e9,stroke:#4caf50\n",
    "    style T4 fill:#e8f5e9,stroke:#4caf50\n",
    "</div>\n",
    "<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>\n",
    "\n",
    "### Job Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Task types** | Notebooks, Python scripts, SQL, JARs, DLT pipelines |\n",
    "| **Dependencies** | Sequential, parallel, conditional execution |\n",
    "| **Triggers** | Cron schedule, file arrival, continuous |\n",
    "| **Parameters** | Pass values between tasks, use widgets |\n",
    "| **Clusters** | Job clusters (ephemeral) or all-purpose (shared) |\n",
    "| **Retries** | Automatic retry on failure with backoff |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "faf74c39-9a9b-47c0-942c-0effe60e55ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating a Lakeflow Job\n",
    "\n",
    "<div class=\"code-block\" data-language=\"python\">\n",
    "# Using Databricks SDK to create a job programmatically\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import Task, NotebookTask, JobCluster\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create job with multiple tasks\n",
    "job = w.jobs.create(\n",
    "    name=\"ETL_Pipeline_Migration\",\n",
    "    tasks=[\n",
    "        Task(\n",
    "            task_key=\"extract\",\n",
    "            notebook_task=NotebookTask(\n",
    "                notebook_path=\"/Repos/team/project/01_extract\",\n",
    "                base_parameters={\"source_table\": \"customers\"}\n",
    "            ),\n",
    "            job_cluster_key=\"etl_cluster\"\n",
    "        ),\n",
    "        Task(\n",
    "            task_key=\"transform\",\n",
    "            depends_on=[{\"task_key\": \"extract\"}],\n",
    "            notebook_task=NotebookTask(\n",
    "                notebook_path=\"/Repos/team/project/02_transform\"\n",
    "            ),\n",
    "            job_cluster_key=\"etl_cluster\"\n",
    "        ),\n",
    "        Task(\n",
    "            task_key=\"load\",\n",
    "            depends_on=[{\"task_key\": \"transform\"}],\n",
    "            notebook_task=NotebookTask(\n",
    "                notebook_path=\"/Repos/team/project/03_load\"\n",
    "            ),\n",
    "            job_cluster_key=\"etl_cluster\"\n",
    "        )\n",
    "    ],\n",
    "    job_clusters=[\n",
    "        JobCluster(\n",
    "            job_cluster_key=\"etl_cluster\",\n",
    "            new_cluster={\n",
    "                \"spark_version\": \"14.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"i3.xlarge\",\n",
    "                \"num_workers\": 2\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    schedule={\n",
    "        \"quartz_cron_expression\": \"0 0 6 * * ?\",  # Daily at 6 AM\n",
    "        \"timezone_id\": \"America/New_York\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created job: {job.job_id}\")\n",
    "</div>\n",
    "\n",
    "<link href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css\" rel=\"stylesheet\" />\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "(function() {\n",
    "    document.querySelectorAll('.code-block').forEach(function(block) {\n",
    "        var lang = block.getAttribute('data-language') || 'python';\n",
    "        var code = block.textContent.trim();\n",
    "        var id = 'code-' + Math.random().toString(36).substr(2, 9);\n",
    "        \n",
    "        block.innerHTML = \n",
    "            '<div style=\"position:relative;margin:16px 0;\">' +\n",
    "                '<button class=\"copy-btn\" style=\"position:absolute;top:8px;right:8px;padding:4px 12px;font-size:12px;background:#ddd;color:#333;border:1px solid #ccc;border-radius:4px;cursor:pointer;z-index:10;\">Copy</button>' +\n",
    "                '<pre style=\"background:#f8f8f8;border-radius:8px;padding:16px;padding-top:40px;overflow-x:auto;margin:0;border:1px solid #e0e0e0;\"><code id=\"' + id + '\" class=\"language-' + lang + '\" style=\"font-family:Consolas,Monaco,monospace;font-size:14px;\"></code></pre>' +\n",
    "            '</div>';\n",
    "        \n",
    "        var codeEl = document.getElementById(id);\n",
    "        codeEl.textContent = code;\n",
    "        Prism.highlightElement(codeEl);\n",
    "        \n",
    "        block.querySelector('.copy-btn').onclick = function() {\n",
    "            var t = document.createElement('textarea');\n",
    "            t.value = code;\n",
    "            document.body.appendChild(t);\n",
    "            t.select();\n",
    "            document.execCommand('copy');\n",
    "            document.body.removeChild(t);\n",
    "            this.textContent = '✓ Copied!';\n",
    "            setTimeout(() => this.textContent = 'Copy', 2000);\n",
    "        };\n",
    "    });\n",
    "})();\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "31277bb4-39a5-4870-83db-ed4d3efc6d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Declarative Pipelines (DLT)\n",
    "\n",
    "Spark Declarative Pipelines provides declarative ETL with built-in data quality controls, automatic dependency management, and Delta Lake optimization.\n",
    "\n",
    "<br />\n",
    "<div class=\"mermaid\">\n",
    "flowchart LR\n",
    "    subgraph DLT[\"Spark Declarative Pipeline\"]\n",
    "        RAW[\"<b>Bronze</b><br/><i>@dlt.table</i>\"]\n",
    "        CLEAN[\"<b>Silver</b><br/><i>@dlt.table</i><br/><i>expectations</i>\"]\n",
    "        AGG[\"<b>Gold</b><br/><i>@dlt.table</i>\"]\n",
    "    end\n",
    "    SOURCE[\"<b>Source</b><br/><i>Files, Streams</i>\"] --> RAW\n",
    "    RAW --> CLEAN --> AGG\n",
    "    AGG --> CONSUME[\"<b>Consumers</b><br/><i>BI, ML, Apps</i>\"]\n",
    "    style DLT fill:#fff,stroke:#FF3621,stroke-width:2px\n",
    "    style RAW fill:#cd7f32,stroke:#8b4513,color:#fff\n",
    "    style CLEAN fill:#c0c0c0,stroke:#808080,color:#000\n",
    "    style AGG fill:#ffd700,stroke:#b8860b,color:#000\n",
    "</div>\n",
    "<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>\n",
    "\n",
    "### DLT Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Expectations** | Data quality rules with actions (warn, drop, fail) |\n",
    "| **Auto-dependency** | Automatic lineage and execution order |\n",
    "| **Incremental** | Efficient processing of new/changed data |\n",
    "| **CDC support** | `APPLY CHANGES INTO` for SCD Type 1 and 2 |\n",
    "| **Monitoring** | Built-in data quality metrics and lineage |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "b5797a11-3086-44a8-832b-3860767cec4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Pipeline Example\n",
    "\n",
    "<div class=\"code-block\" data-language=\"python\">\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Bronze: Raw ingestion\n",
    "@dlt.table(\n",
    "    name=\"bronze_orders\",\n",
    "    comment=\"Raw orders from source system\"\n",
    ")\n",
    "def bronze_orders():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .load(\"/landing/orders/\")\n",
    "    )\n",
    "\n",
    "# Silver: Cleansed with expectations\n",
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Cleansed orders with quality rules\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_date\", \"order_date IS NOT NULL\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_orders\")\n",
    "        .select(\n",
    "            col(\"order_id\"),\n",
    "            col(\"customer_id\"),\n",
    "            col(\"order_date\").cast(\"date\"),\n",
    "            col(\"amount\").cast(\"decimal(18,2)\"),\n",
    "            current_timestamp().alias(\"processed_at\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Gold: Business aggregation\n",
    "@dlt.table(\n",
    "    name=\"gold_daily_sales\",\n",
    "    comment=\"Daily sales summary\"\n",
    ")\n",
    "def gold_daily_sales():\n",
    "    return (\n",
    "        dlt.read(\"silver_orders\")\n",
    "        .groupBy(\"order_date\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"order_count\"),\n",
    "            sum(\"amount\").alias(\"total_amount\")\n",
    "        )\n",
    "    )\n",
    "</div>\n",
    "\n",
    "<link href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css\" rel=\"stylesheet\" />\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "(function() {\n",
    "    document.querySelectorAll('.code-block').forEach(function(block) {\n",
    "        var lang = block.getAttribute('data-language') || 'python';\n",
    "        var code = block.textContent.trim();\n",
    "        var id = 'code-' + Math.random().toString(36).substr(2, 9);\n",
    "        \n",
    "        block.innerHTML = \n",
    "            '<div style=\"position:relative;margin:16px 0;\">' +\n",
    "                '<button class=\"copy-btn\" style=\"position:absolute;top:8px;right:8px;padding:4px 12px;font-size:12px;background:#ddd;color:#333;border:1px solid #ccc;border-radius:4px;cursor:pointer;z-index:10;\">Copy</button>' +\n",
    "                '<pre style=\"background:#f8f8f8;border-radius:8px;padding:16px;padding-top:40px;overflow-x:auto;margin:0;border:1px solid #e0e0e0;\"><code id=\"' + id + '\" class=\"language-' + lang + '\" style=\"font-family:Consolas,Monaco,monospace;font-size:14px;\"></code></pre>' +\n",
    "            '</div>';\n",
    "        \n",
    "        var codeEl = document.getElementById(id);\n",
    "        codeEl.textContent = code;\n",
    "        Prism.highlightElement(codeEl);\n",
    "        \n",
    "        block.querySelector('.copy-btn').onclick = function() {\n",
    "            var t = document.createElement('textarea');\n",
    "            t.value = code;\n",
    "            document.body.appendChild(t);\n",
    "            t.select();\n",
    "            document.execCommand('copy');\n",
    "            document.body.removeChild(t);\n",
    "            this.textContent = '✓ Copied!';\n",
    "            setTimeout(() => this.textContent = 'Copy', 2000);\n",
    "        };\n",
    "    });\n",
    "})();\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "1f8f7a38-76ca-413c-8a0b-a80ac61afca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## External Orchestrator Integration\n",
    "\n",
    "If you use external orchestrators like Apache Airflow or Azure Data Factory, integrate them with Databricks using APIs or native connectors.\n",
    "\n",
    "<br />\n",
    "<div class=\"mermaid\">\n",
    "flowchart TB\n",
    "    subgraph EXTERNAL[\"External Orchestrator\"]\n",
    "        SCHEDULE[\"<b>Schedule</b><br/><i>Cron, Event</i>\"]\n",
    "        DAG[\"<b>DAG / Pipeline</b><br/><i>Tasks, Dependencies</i>\"]\n",
    "    end\n",
    "    subgraph DATABRICKS[\"Databricks\"]\n",
    "        API[\"<b>Jobs API</b><br/><i>REST / SDK</i>\"]\n",
    "        JOBS[\"<b>Lakeflow Jobs</b>\"]\n",
    "        DLT[\"<b>DLT Pipelines</b>\"]\n",
    "    end\n",
    "    SCHEDULE --> DAG\n",
    "    DAG -->|\"Trigger\"| API\n",
    "    API --> JOBS\n",
    "    API --> DLT\n",
    "    style EXTERNAL fill:#fff3e0,stroke:#ff9800\n",
    "    style DATABRICKS fill:#e8f5e9,stroke:#4caf50\n",
    "</div>\n",
    "<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>\n",
    "\n",
    "### Integration Options\n",
    "\n",
    "| Orchestrator | Integration Method |\n",
    "|--------------|--------------------|\n",
    "| **Apache Airflow** | `DatabricksRunNowOperator`, `DatabricksSubmitRunOperator` |\n",
    "| **Azure Data Factory** | Databricks Linked Service, Notebook Activity |\n",
    "| **AWS Step Functions** | Lambda + Databricks SDK |\n",
    "| **Prefect / Dagster** | Databricks SDK blocks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "6df7453a-269b-4f2b-acd4-cc4ef82c52ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Airflow Integration Example\n",
    "\n",
    "<div class=\"code-block\" data-language=\"python\">\n",
    "from airflow import DAG\n",
    "from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"etl_migration_pipeline\",\n",
    "    schedule_interval=\"0 6 * * *\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # Trigger existing Databricks job\n",
    "    run_etl_job = DatabricksRunNowOperator(\n",
    "        task_id=\"run_etl_job\",\n",
    "        databricks_conn_id=\"databricks_default\",\n",
    "        job_id=12345,  # Lakeflow Job ID\n",
    "        notebook_params={\n",
    "            \"run_date\": \"{{ ds }}\",\n",
    "            \"source_schema\": \"bronze\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Chain with other tasks\n",
    "    run_etl_job\n",
    "</div>\n",
    "\n",
    "<link href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css\" rel=\"stylesheet\" />\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "(function() {\n",
    "    document.querySelectorAll('.code-block').forEach(function(block) {\n",
    "        var lang = block.getAttribute('data-language') || 'python';\n",
    "        var code = block.textContent.trim();\n",
    "        var id = 'code-' + Math.random().toString(36).substr(2, 9);\n",
    "        \n",
    "        block.innerHTML = \n",
    "            '<div style=\"position:relative;margin:16px 0;\">' +\n",
    "                '<button class=\"copy-btn\" style=\"position:absolute;top:8px;right:8px;padding:4px 12px;font-size:12px;background:#ddd;color:#333;border:1px solid #ccc;border-radius:4px;cursor:pointer;z-index:10;\">Copy</button>' +\n",
    "                '<pre style=\"background:#f8f8f8;border-radius:8px;padding:16px;padding-top:40px;overflow-x:auto;margin:0;border:1px solid #e0e0e0;\"><code id=\"' + id + '\" class=\"language-' + lang + '\" style=\"font-family:Consolas,Monaco,monospace;font-size:14px;\"></code></pre>' +\n",
    "            '</div>';\n",
    "        \n",
    "        var codeEl = document.getElementById(id);\n",
    "        codeEl.textContent = code;\n",
    "        Prism.highlightElement(codeEl);\n",
    "        \n",
    "        block.querySelector('.copy-btn').onclick = function() {\n",
    "            var t = document.createElement('textarea');\n",
    "            t.value = code;\n",
    "            document.body.appendChild(t);\n",
    "            t.select();\n",
    "            document.execCommand('copy');\n",
    "            document.body.removeChild(t);\n",
    "            this.textContent = '✓ Copied!';\n",
    "            setTimeout(() => this.textContent = 'Copy', 2000);\n",
    "        };\n",
    "    });\n",
    "})();\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "0dc51530-84ed-428e-aca3-4224940a0117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitoring and Alerting\n",
    "\n",
    "Implement comprehensive monitoring for migrated pipelines to ensure reliability.\n",
    "\n",
    "### Monitoring Options\n",
    "\n",
    "| Component | Tool | Metrics |\n",
    "|-----------|------|--------|\n",
    "| **Job runs** | Lakeflow Jobs UI | Duration, status, failures |\n",
    "| **DLT quality** | DLT Event Log | Expectation pass/fail rates |\n",
    "| **Cluster metrics** | Ganglia, Spark UI | CPU, memory, shuffle |\n",
    "| **Custom alerts** | Databricks Alerts | SQL-based conditions |\n",
    "| **External** | Datadog, Splunk | Unified observability |\n",
    "\n",
    "### Alert Configuration\n",
    "\n",
    "<div class=\"code-block\" data-language=\"sql\">\n",
    "-- Create alert for job failures\n",
    "-- (Configure via Databricks SQL Alerts UI)\n",
    "\n",
    "-- Query to detect failed jobs in last hour\n",
    "SELECT \n",
    "    job_id,\n",
    "    run_id,\n",
    "    state.result_state,\n",
    "    state.state_message,\n",
    "    end_time\n",
    "FROM system.lakeflow.job_runs\n",
    "WHERE state.result_state = 'FAILED'\n",
    "  AND end_time > current_timestamp() - INTERVAL 1 HOUR\n",
    "ORDER BY end_time DESC;\n",
    "</div>\n",
    "\n",
    "<link href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css\" rel=\"stylesheet\" />\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n",
    "\n",
    "<script>\n",
    "(function() {\n",
    "    document.querySelectorAll('.code-block').forEach(function(block) {\n",
    "        var lang = block.getAttribute('data-language') || 'sql';\n",
    "        var code = block.textContent.trim();\n",
    "        var id = 'code-' + Math.random().toString(36).substr(2, 9);\n",
    "        \n",
    "        block.innerHTML = \n",
    "            '<div style=\"position:relative;margin:16px 0;\">' +\n",
    "                '<button class=\"copy-btn\" style=\"position:absolute;top:8px;right:8px;padding:4px 12px;font-size:12px;background:#ddd;color:#333;border:1px solid #ccc;border-radius:4px;cursor:pointer;z-index:10;\">Copy</button>' +\n",
    "                '<pre style=\"background:#f8f8f8;border-radius:8px;padding:16px;padding-top:40px;overflow-x:auto;margin:0;border:1px solid #e0e0e0;\"><code id=\"' + id + '\" class=\"language-' + lang + '\" style=\"font-family:Consolas,Monaco,monospace;font-size:14px;\"></code></pre>' +\n",
    "            '</div>';\n",
    "        \n",
    "        var codeEl = document.getElementById(id);\n",
    "        codeEl.textContent = code;\n",
    "        Prism.highlightElement(codeEl);\n",
    "        \n",
    "        block.querySelector('.copy-btn').onclick = function() {\n",
    "            var t = document.createElement('textarea');\n",
    "            t.value = code;\n",
    "            document.body.appendChild(t);\n",
    "            t.select();\n",
    "            document.execCommand('copy');\n",
    "            document.body.removeChild(t);\n",
    "            this.textContent = '✓ Copied!';\n",
    "            setTimeout(() => this.textContent = 'Copy', 2000);\n",
    "        };\n",
    "    });\n",
    "})();\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a76374e7-f2f2-4c55-a041-a78982bdab0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Migration Pattern: {SOURCE_PLATFORM} to Databricks\n",
    "\n",
    "Map your existing {SOURCE_PLATFORM} scheduling concepts to Databricks equivalents.\n",
    "\n",
    "### Concept Mapping\n",
    "\n",
    "| {SOURCE_PLATFORM} Concept | Databricks Equivalent |\n",
    "|---------------------------|----------------------|\n",
    "| Scheduled Task | Lakeflow Job with cron trigger |\n",
    "| Stored Procedure | Notebook task or SQL Procedure |\n",
    "| Task Dependencies | Job task dependencies |\n",
    "| Streams/Change Tracking | Auto Loader, CDC |\n",
    "| Alerts | Databricks Alerts |\n",
    "\n",
    "### Migration Steps\n",
    "\n",
    "1. **Inventory** - Catalog all scheduled jobs and dependencies\n",
    "2. **Convert logic** - Port SQL/procedures (see [3.4 - SQL and Code Conversion]($./3.4 - SQL and Code Conversion))\n",
    "3. **Create jobs** - Build Lakeflow Jobs or DLT pipelines\n",
    "4. **Configure triggers** - Set schedules matching source\n",
    "5. **Enable monitoring** - Set up alerts and dashboards\n",
    "6. **Test end-to-end** - Validate outputs match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "539e4bc8-52be-4407-b971-3cdc9eaf9149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Orchestration Checklist\n",
    "\n",
    "- [ ] Inventory all source scheduled jobs and dependencies\n",
    "- [ ] Choose orchestration tool (Lakeflow Jobs, DLT, external)\n",
    "- [ ] Convert job logic to notebooks/pipelines\n",
    "- [ ] Configure triggers and schedules\n",
    "- [ ] Set up monitoring and alerting\n",
    "- [ ] Test in parallel with source before cutover\n",
    "- [ ] Document runbooks for operations\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "| Principle | Why It Matters |\n",
    "|-----------|----------------|\n",
    "| **Use Lakeflow Jobs for orchestration** | Native integration, monitoring, retries |\n",
    "| **Adopt DLT for ETL** | Declarative, quality controls, auto-optimization |\n",
    "| **Monitor proactively** | Catch failures before business impact |\n",
    "| **Test thoroughly** | Run parallel before decommissioning source |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With pipelines migrated, proceed to cutover planning:\n",
    "\n",
    "- [**Module 04 - Cutover**]($../04 - Cutover/4.1 - Cutover Planning Overview) - Transition planning and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "23b8fc6b-2f5a-48b2-a774-e6cb37080134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"color: #FF3621; font-weight: bold; font-size: 2em; margin-bottom: 12px;\">COURSE DEVELOPER (remove before publishing)</div>\n",
    "\n",
    "### Template Customization\n",
    "\n",
    "**Placeholders to replace:**\n",
    "- `{SOURCE_PLATFORM}` - Source platform name (Snowflake, BigQuery, Redshift, Teradata)\n",
    "\n",
    "**Platform-specific additions required:**\n",
    "- Add platform-specific scheduling concept mapping (e.g., Snowflake Tasks, dbt Cloud)\n",
    "- Include platform-specific CDC/streaming patterns\n",
    "- Add platform-specific monitoring equivalents\n",
    "- Document any native connectors available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc332503-51f8-4149-9119-bbba42f51754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3.5 - Pipeline and Orchestration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
