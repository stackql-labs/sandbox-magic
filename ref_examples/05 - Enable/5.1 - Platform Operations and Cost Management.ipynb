{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 8px 16px; background: #F8F9FA; border-bottom: 2px solid #E0E0E0; margin: 0; line-height: 1;\">\n    <div style=\"font-size: 14px; color: #666;\">\n        <span style=\"font-weight: bold; color: #333;\">{SOURCE_PLATFORM} ‚Üí Databricks Migration</span>\n        <span style=\"margin-left: 8px; color: #999;\">|</span>\n        <span style=\"margin-left: 8px;\">05 - Enable</span>\n    </div>\n    <div style=\"display: flex; align-items: center; gap: 8px;\">\n        <img src=\"https://cdn.simpleicons.org/snowflake/29B5E8\" width=\"24\" height=\"24\"/>\n        <span style=\"color: #999; font-size: 16px;\">‚Üí</span>\n        <img src=\"https://cdn.simpleicons.org/databricks/FF3621\" width=\"24\" height=\"24\"/>\n    </div>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img\n    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n    alt=\"Databricks Learning\"\n  >\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "# Platform Operations and Cost Management\n\n## Overview\n\n**Critical Context:** Earlier modules focused on *building* your Databricks platform - migrating data, converting code, and establishing governance. This module shifts focus to *operationalizing* and *scaling* the platform for ongoing production use.\n\nPost-migration success requires robust operational practices: optimizing compute for production concurrency, implementing cost controls and attribution, integrating observability tooling, establishing incident response workflows, and managing infrastructure as code. These capabilities transform a successful migration into a sustainable, production-grade data platform.\n\n## Learning Objectives\n\nBy the end of this lesson, you will be able to:\n- Configure and tune Serverless SQL Warehouses for production concurrency\n- Implement billing tags and DBU cost attribution using `system.billing.usage`\n- Integrate Databricks with enterprise observability platforms (CloudWatch, Azure Monitor, Splunk)\n- Design alert policies and incident response workflows\n- Manage platform infrastructure as code using Terraform and Databricks Asset Bundles"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Production Compute Optimization\n\n### Serverless SQL Warehouse Tuning\n\nServerless SQL Warehouses provide instant startup, automatic scaling, and minimal operational overhead - ideal for production analytics workloads. However, achieving optimal cost and performance requires understanding configuration options and scaling behavior.\n\n<br />\n<div class=\"mermaid\">\ngraph LR\n    subgraph \"User Request Lifecycle\"\n        A[Query Submitted] --> B{Warehouse<br/>Running?}\n        B -->|No| C[Instant Startup<br/>&lt;10s]\n        B -->|Yes| D[Route to Cluster]\n        C --> D\n        D --> E{Capacity<br/>Available?}\n        E -->|Yes| F[Execute Query]\n        E -->|No| G[Auto-Scale<br/>Add Cluster]\n        G --> F\n        F --> H[Return Results]\n    end\n    style C fill:#E3F2FD\n    style G fill:#FFF3E0\n    style F fill:#E8F5E9\n</div>\n<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Key Configuration Parameters\n\n| Parameter | Purpose | Production Considerations |\n|-----------|---------|---------------------------|\n| **Warehouse Size** | Base compute capacity (X-Small to 4X-Large) | Start with Medium; monitor queue time and scaling events; upsize if consistent saturation |\n| **Max Clusters** | Auto-scaling limit (1-10 for Serverless) | Set to 3-5x base capacity for BI workloads; monitor peak concurrency patterns |\n| **Auto-stop** | Idle timeout before shutdown | 10-15 minutes for production workloads; balance cost vs. startup delay |\n| **Scaling Policy** | How aggressively to scale (Economy, Standard, Performance) | Use Standard for predictable workloads; Performance for low-latency SLAs |\n| **Catalog** | Default Unity Catalog for queries | Align with governance model; users can override with USE CATALOG |\n| **Query History** | Retention for query logs | Maximum retention for compliance and troubleshooting |\n| **Spot Instance Policy** | Use spot/preemptible instances | Enable for non-critical workloads to reduce costs by ~70% |\n\n<div style=\"background-color: #E3F2FD; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 16px 0;\">\n<strong>üí° Recommendation: Right-Sizing Strategy</strong><br/><br/>\nStart conservative (Small/Medium, max clusters = 2-3) and scale based on observed metrics. Over-provisioning wastes budget; under-provisioning creates user frustration. Monitor these signals:\n<ul>\n<li><strong>Query Queue Time</strong>: Sustained queuing indicates insufficient capacity</li>\n<li><strong>Cluster Utilization</strong>: Target 60-80% average utilization during business hours</li>\n<li><strong>Auto-scaling Events</strong>: Frequent scaling suggests base size too small or max clusters too low</li>\n<li><strong>Query Duration P95/P99</strong>: Percentile latencies reveal user experience degradation</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Concurrency Patterns and Anti-Patterns\n\n**High-Concurrency BI Workloads**\n\n| Pattern | Description | Configuration |\n|---------|-------------|---------------|\n| **Dashboard Refresh** | 50-200 concurrent queries during business hours; predictable schedule | Medium/Large warehouse, max clusters = 5-8, Standard scaling policy |\n| **Ad-Hoc Exploration** | 10-30 concurrent users; unpredictable query patterns | Medium warehouse, max clusters = 3-5, Economy scaling for cost control |\n| **Executive Reporting** | Low concurrency (&lt;10), low-latency SLA (&lt;5s) | Small/Medium warehouse, max clusters = 2, Performance scaling, dedicated warehouse |\n| **Embedded Analytics** | Customer-facing dashboards; variable concurrency; uptime SLA | Large warehouse, max clusters = 8-10, Performance scaling, always-on (no auto-stop) |\n\n**Anti-Patterns to Avoid**\n\n<div style=\"background-color: #FFEBEE; border-left: 4px solid #F44336; padding: 12px 16px; margin: 16px 0;\">\n<strong>‚ö†Ô∏è Common Mistakes</strong><br/><br/>\n<ul>\n<li><strong>Single Warehouse for All Workloads</strong>: Mixing ETL, BI, and ad-hoc creates resource contention and unpredictable costs. Use dedicated warehouses per workload class with appropriate tagging.</li>\n<li><strong>Aggressive Auto-Stop (&lt;5 min)</strong>: Causes frequent startup delays. For production BI, 10-15 min idle timeout balances cost and UX.</li>\n<li><strong>Max Clusters = 1</strong>: Eliminates concurrency benefits. Serverless is designed to scale; allow at least 2-3 clusters.</li>\n<li><strong>Ignoring Query Patterns</strong>: Not all queries are equal. Segment users: power users (complex, long-running) vs. dashboards (simple, frequent).</li>\n<li><strong>No Tagging Strategy</strong>: Without billing tags, cost attribution is impossible. Tag every warehouse with cost center, team, environment.</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Monitoring Serverless Performance\n\nUse Databricks system tables to monitor warehouse performance and identify optimization opportunities:\n\n<div class=\"code-block\" data-language=\"sql\">-- Query performance metrics by warehouse\nSELECT\n  warehouse_id,\n  warehouse_name,\n  date_trunc('hour', start_time) AS hour,\n  COUNT(*) AS query_count,\n  ROUND(AVG(total_duration_ms) / 1000, 2) AS avg_duration_seconds,\n  ROUND(PERCENTILE(total_duration_ms, 0.95) / 1000, 2) AS p95_duration_seconds,\n  ROUND(PERCENTILE(total_duration_ms, 0.99) / 1000, 2) AS p99_duration_seconds,\n  SUM(CASE WHEN error_message IS NOT NULL THEN 1 ELSE 0 END) AS error_count,\n  ROUND(AVG(queued_duration_ms) / 1000, 2) AS avg_queue_time_seconds\nFROM\n  system.query.history\nWHERE\n  start_time >= CURRENT_DATE - INTERVAL 7 DAYS\n  AND warehouse_id IS NOT NULL\nGROUP BY\n  warehouse_id, warehouse_name, hour\nORDER BY\n  hour DESC, query_count DESC;</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div class=\"code-block\" data-language=\"sql\">-- Identify queries with high queue times (capacity issues)\nSELECT\n  query_id,\n  warehouse_name,\n  user_name,\n  start_time,\n  ROUND(queued_duration_ms / 1000, 2) AS queue_time_seconds,\n  ROUND(total_duration_ms / 1000, 2) AS total_duration_seconds,\n  statement_type,\n  LEFT(query_text, 100) AS query_preview\nFROM\n  system.query.history\nWHERE\n  start_time >= CURRENT_DATE - INTERVAL 3 DAYS\n  AND queued_duration_ms > 5000  -- Queries queued >5 seconds\nORDER BY\n  queued_duration_ms DESC\nLIMIT 50;</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Classic vs. Serverless SQL Warehouses\n\n| Consideration | Classic SQL Warehouse | Serverless SQL Warehouse |\n|---------------|------------------------|--------------------------|\n| **Startup Time** | 3-5 minutes (cold start) | &lt;10 seconds |\n| **Scaling Speed** | 2-3 minutes to add cluster | 10-20 seconds to add capacity |\n| **Infrastructure** | Runs in customer's cloud account (VPC/VNet) | Runs in Databricks-managed infrastructure |\n| **Private Connectivity** | Supports Private Link, VPC peering | Limited private connectivity options ¬π |\n| **Customization** | Cluster policies, init scripts, custom libraries | Limited customization; managed environment |\n| **Cost Model** | Pay for uptime (idle time = wasted cost) | Pay for query execution only; near-zero idle cost |\n| **Data Access** | Direct access to customer storage via IAM/SPN | Accesses storage via secure proxy |\n| **Compliance** | Data plane in customer account (regulatory advantage) | Data plane in Databricks account (simplified ops) |\n| **Best For** | Strict data residency, custom networking, regulated industries | General-purpose analytics, cost optimization, developer productivity |\n\n<div style=\"font-size: 0.85em; color: #555; margin-top: 8px;\">\n¬π Serverless SQL Warehouses support Private Link on AWS and Azure with additional configuration. Consult Databricks documentation for latest capabilities.\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Cost Management and Attribution\n\n### The Challenge: Multi-Tenant Cost Allocation\n\nPost-migration, Databricks typically serves multiple teams, cost centers, and projects on a shared infrastructure. Without proper tagging and attribution, answering \"What did Team X spend last month?\" becomes impossible.\n\n<br />\n<div class=\"mermaid\">\ngraph TB\n    subgraph \"Cost Attribution Flow\"\n        A[Databricks Usage<br/>DBUs Consumed] --> B[system.billing.usage<br/>Raw Usage Data]\n        B --> C[Tag Enrichment<br/>cost_center, team, project]\n        C --> D[Cost Allocation Logic<br/>DBUs √ó Rate]\n        D --> E[Chargeback Reports<br/>FinOps Dashboards]\n        E --> F[Budget Alerts<br/>Anomaly Detection]\n    end\n    style B fill:#FFF3E0\n    style C fill:#E3F2FD\n    style D fill:#F3E5F5\n    style E fill:#E8F5E9\n</div>\n<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Billing Tags: The Foundation of Cost Attribution\n\n**Tag Strategy**\n\nEvery compute resource (warehouse, cluster, DLT pipeline, model endpoint) should have consistent tags:\n\n| Tag Key | Purpose | Example Values |\n|---------|---------|----------------|\n| `cost_center` | Finance chargeback | `\"marketing\"`, `\"engineering\"`, `\"data_science\"` |\n| `team` | Team-level attribution | `\"growth_analytics\"`, `\"ml_platform\"`, `\"bi_reporting\"` |\n| `environment` | Separate dev/staging/prod costs | `\"dev\"`, `\"staging\"`, `\"prod\"` |\n| `project` | Project-specific tracking | `\"customer_360\"`, `\"fraud_detection\"`, `\"recommendation_engine\"` |\n| `owner` | Responsible individual/group | `\"jane.doe@company.com\"`, `\"data-platform-team\"` |\n| `budget_code` | Finance system integration | `\"FY26-Q1-ANALYTICS\"`, `\"CAPEX-2026-ML\"` |\n\n<div style=\"background-color: #E3F2FD; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 16px 0;\">\n<strong>üí° Recommendation: Enforce Tags with Cluster Policies</strong><br/><br/>\nUse Databricks Cluster Policies to require specific tags before users can create compute resources. This ensures complete coverage and prevents untagged spend from becoming \"dark matter\" in your billing data.\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Applying Tags to Compute Resources\n\n**SQL Warehouse Tags** (via UI or API):\n\n<div class=\"code-block\" data-language=\"python\"># Using Databricks SDK for Python\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\n# Create or update SQL Warehouse with tags\nwarehouse = w.warehouses.create(\n    name=\"analytics_warehouse_prod\",\n    cluster_size=\"Medium\",\n    max_num_clusters=5,\n    auto_stop_mins=15,\n    tags={\n        \"cost_center\": \"analytics\",\n        \"team\": \"bi_reporting\",\n        \"environment\": \"prod\",\n        \"owner\": \"bi-team@company.com\"\n    }\n)</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Cluster Tags** (via cluster configuration):\n\n<div class=\"code-block\" data-language=\"python\"># Using Databricks SDK for Python\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.compute import ClusterSpec\n\nw = WorkspaceClient()\n\ncluster = w.clusters.create(\n    cluster_name=\"etl_job_cluster\",\n    spark_version=w.clusters.select_spark_version(latest=True),\n    node_type_id=w.clusters.select_node_type(local_disk=True),\n    num_workers=4,\n    autotermination_minutes=30,\n    custom_tags={\n        \"cost_center\": \"data_engineering\",\n        \"team\": \"etl_platform\",\n        \"environment\": \"prod\",\n        \"project\": \"daily_aggregation\"\n    }\n).result()</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Using `system.billing.usage` for Cost Attribution\n\nThe `system.billing.usage` table is the authoritative source for DBU consumption and cost data. It's updated daily with all usage across the account.\n\n**Table Schema Overview:**\n\n| Column | Type | Description |\n|--------|------|-------------|\n| `account_id` | STRING | Databricks account identifier |\n| `workspace_id` | STRING | Workspace where usage occurred |\n| `sku_name` | STRING | Product SKU (e.g., `STANDARD_ALL_PURPOSE_COMPUTE`, `SQL_COMPUTE`) |\n| `cloud` | STRING | Cloud provider (`AWS`, `AZURE`, `GCP`) |\n| `usage_start_time` | TIMESTAMP | Start of usage period |\n| `usage_end_time` | TIMESTAMP | End of usage period |\n| `usage_date` | DATE | Date of usage (for partitioning) |\n| `custom_tags` | MAP&lt;STRING, STRING&gt; | User-defined tags from compute resources |\n| `usage_unit` | STRING | Unit of measurement (typically `DBU`) |\n| `usage_quantity` | DECIMAL | Amount of DBUs consumed |\n| `usage_metadata` | STRUCT | Additional context (cluster_id, warehouse_id, job_id, etc.) |\n| `billing_origin_product` | STRING | Origin product (e.g., `JOBS`, `SQL`, `NOTEBOOKS`) |\n\n<div class=\"code-block\" data-language=\"sql\">-- Daily cost by cost center (requires DBU rate mapping)\nSELECT\n  usage_date,\n  custom_tags['cost_center'] AS cost_center,\n  sku_name,\n  SUM(usage_quantity) AS total_dbus,\n  -- Apply your negotiated DBU rates here\n  ROUND(SUM(usage_quantity) *\n    CASE\n      WHEN sku_name LIKE '%SERVERLESS%' THEN 0.70  -- Example rate\n      WHEN sku_name LIKE '%ALL_PURPOSE%' THEN 0.55\n      WHEN sku_name LIKE '%JOBS%' THEN 0.15\n      ELSE 0.40\n    END, 2) AS estimated_cost_usd\nFROM\n  system.billing.usage\nWHERE\n  usage_date >= CURRENT_DATE - INTERVAL 30 DAYS\n  AND custom_tags['cost_center'] IS NOT NULL\nGROUP BY\n  usage_date, cost_center, sku_name\nORDER BY\n  usage_date DESC, total_dbus DESC;</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div class=\"code-block\" data-language=\"sql\">-- Identify untagged usage (cost attribution gaps)\nSELECT\n  usage_date,\n  sku_name,\n  billing_origin_product,\n  usage_metadata.cluster_id,\n  usage_metadata.warehouse_id,\n  SUM(usage_quantity) AS untagged_dbus\nFROM\n  system.billing.usage\nWHERE\n  usage_date >= CURRENT_DATE - INTERVAL 7 DAYS\n  AND (\n    custom_tags IS NULL\n    OR custom_tags['cost_center'] IS NULL\n    OR custom_tags['team'] IS NULL\n  )\nGROUP BY\n  usage_date, sku_name, billing_origin_product,\n  usage_metadata.cluster_id, usage_metadata.warehouse_id\nORDER BY\n  untagged_dbus DESC;</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div class=\"code-block\" data-language=\"sql\">-- Top 10 most expensive workloads by team (past 30 days)\nSELECT\n  custom_tags['team'] AS team,\n  custom_tags['project'] AS project,\n  sku_name,\n  COUNT(DISTINCT usage_metadata.cluster_id) AS unique_clusters,\n  SUM(usage_quantity) AS total_dbus,\n  ROUND(SUM(usage_quantity) * 0.40, 2) AS estimated_cost_usd  -- Adjust rate\nFROM\n  system.billing.usage\nWHERE\n  usage_date >= CURRENT_DATE - INTERVAL 30 DAYS\n  AND custom_tags['team'] IS NOT NULL\nGROUP BY\n  team, project, sku_name\nORDER BY\n  total_dbus DESC\nLIMIT 10;</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Budget Monitoring and Alerts\n\nBuild dashboards and alerts on `system.billing.usage` to proactively manage costs:\n\n| Alert Type | Logic | Action |\n|------------|-------|--------|\n| **Budget Threshold** | Team DBU spend > 80% of monthly budget | Email team lead; require approval for new clusters |\n| **Anomaly Detection** | Daily spend > 2œÉ from 30-day average | Investigate for runaway jobs or misconfiguration |\n| **Untagged Usage** | Untagged DBUs > 5% of total usage | Notify platform team; enforce tagging policies |\n| **Idle Compute** | Clusters running >24 hours with zero queries | Auto-terminate; notify owner |\n| **Cost Per Query** | Warehouse cost/query > threshold | Investigate inefficient queries; optimize warehouse size |\n\n<div style=\"background-color: #FFF3E0; border-left: 4px solid #FF9800; padding: 12px 16px; margin: 16px 0;\">\n<strong>‚öôÔ∏è Best Practice: Automated Cost Governance</strong><br/><br/>\nCombine <code>system.billing.usage</code> alerts with automated responses:\n<ul>\n<li><strong>Soft Limits</strong>: Email notifications at 75%, 90%, 100% of budget</li>\n<li><strong>Hard Limits</strong>: Use Cluster Policies to restrict instance types or max clusters when budget exceeded</li>\n<li><strong>Weekly Reports</strong>: Scheduled SQL queries that email cost summaries to team leads every Monday</li>\n<li><strong>Chargeback Integration</strong>: Export tagged usage to finance systems (SAP, Oracle Financials) for departmental billing</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Observability and Monitoring Integrations\n\n### The Observability Stack\n\nProduction data platforms require integration with enterprise observability tools for centralized monitoring, alerting, and incident response.\n\n<br />\n<div class=\"mermaid\">\ngraph TB\n    subgraph \"Databricks Platform\"\n        A[Clusters & Warehouses]\n        B[Jobs & Pipelines]\n        C[System Tables]\n        D[Audit Logs]\n    end\n\n    subgraph \"Log Delivery\"\n        E[Diagnostic Logs]\n        F[Audit Log Delivery]\n        G[System Tables Export]\n    end\n\n    subgraph \"Observability Platforms\"\n        H[CloudWatch<br/>AWS]\n        I[Azure Monitor<br/>Log Analytics]\n        J[Splunk<br/>Enterprise]\n        K[Datadog]\n        L[Prometheus/Grafana]\n    end\n\n    A --> E\n    B --> E\n    D --> F\n    C --> G\n\n    E --> H\n    E --> I\n    E --> J\n    F --> H\n    F --> I\n    F --> J\n    G --> K\n    G --> L\n\n    style A fill:#E3F2FD\n    style B fill:#E3F2FD\n    style C fill:#FFF3E0\n    style D fill:#FFF3E0\n    style H fill:#FFE0B2\n    style I fill:#B3E5FC\n    style J fill:#C5E1A5\n</div>\n<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Log Types and Use Cases\n\n| Log Type | Content | Primary Use Cases |\n|----------|---------|-------------------|\n| **Cluster Event Logs** | Cluster lifecycle (start, resize, terminate), driver/executor logs | Debugging job failures, performance tuning, capacity planning |\n| **Audit Logs** | All user actions (logins, data access, permission changes) | Security compliance, access reviews, incident forensics |\n| **Job Run Logs** | Job execution history, task outcomes, error messages | Pipeline monitoring, SLA tracking, failure analysis |\n| **Query History** | SQL query text, execution plans, performance metrics | Query optimization, cost attribution, user behavior analysis |\n| **System Metrics** | CPU, memory, disk I/O, network throughput | Capacity planning, anomaly detection, resource optimization |\n\n### Integration Patterns\n\n**Pattern 1: Log Delivery to Cloud Storage**\n\nDatabricks delivers logs to your cloud storage (S3, ADLS Gen2, GCS), which you then ingest into your observability platform.\n\n<div class=\"code-block\" data-language=\"python\"># Configure audit log delivery using Databricks SDK\nfrom databricks.sdk import AccountClient\nfrom databricks.sdk.service.billing import LogDeliveryConfiguration, LogType, OutputFormat\n\na = AccountClient()\n\n# Create log delivery configuration for audit logs\nlog_delivery = a.log_delivery.create(\n    log_delivery_configuration=LogDeliveryConfiguration(\n        config_name=\"audit_logs_to_s3\",\n        log_type=LogType.AUDIT_LOGS,\n        output_format=OutputFormat.JSON,\n        credentials_id=\"<storage-credential-id>\",\n        storage_configuration_id=\"<storage-config-id>\",\n        workspace_ids_filter=[1234567890123456]  # Optional: specific workspaces\n    )\n)\n\nprint(f\"Log delivery configured: {log_delivery.log_delivery_configuration.config_id}\")</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Pattern 2: Direct CloudWatch Integration (AWS)**\n\nFor AWS deployments, stream logs directly to CloudWatch Logs:\n\n<div class=\"code-block\" data-language=\"python\"># Configure cluster to send logs to CloudWatch\ncluster_config = {\n    \"cluster_name\": \"analytics_cluster\",\n    \"spark_version\": \"14.3.x-scala2.12\",\n    \"node_type_id\": \"i3.xlarge\",\n    \"num_workers\": 4,\n    \"cluster_log_conf\": {\n        \"cloudwatch\": {\n            \"log_group_name\": \"/databricks/clusters\",\n            \"log_stream_name_prefix\": \"analytics-\"\n        }\n    },\n    \"custom_tags\": {\n        \"cost_center\": \"analytics\",\n        \"environment\": \"prod\"\n    }\n}\n\n# Create cluster with CloudWatch logging enabled\nw.clusters.create(**cluster_config)</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Pattern 3: Azure Monitor Integration (Azure)**\n\nFor Azure deployments, integrate with Log Analytics workspace:\n\n<div class=\"code-block\" data-language=\"python\"># Configure cluster to send logs to Azure Log Analytics\ncluster_config = {\n    \"cluster_name\": \"etl_cluster_prod\",\n    \"spark_version\": \"14.3.x-scala2.12\",\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"num_workers\": 8,\n    \"cluster_log_conf\": {\n        \"dbfs\": {\n            \"destination\": \"dbfs:/cluster-logs/etl-prod\"\n        }\n    },\n    \"init_scripts\": [\n        {\n            \"dbfs\": {\n                \"destination\": \"dbfs:/init-scripts/azure-monitor-agent.sh\"\n            }\n        }\n    ]\n}\n\n# Then use Azure Monitor agent to forward logs from DBFS/storage</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Pattern 4: Splunk HTTP Event Collector (HEC)**\n\nFor Splunk environments, use HTTP Event Collector to stream logs:\n\n<div class=\"code-block\" data-language=\"python\"># Python script to forward Databricks logs to Splunk HEC\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nSPLUNK_HEC_URL = \"https://splunk.company.com:8088/services/collector/event\"\nSPLUNK_HEC_TOKEN = \"YOUR-HEC-TOKEN\"\n\ndef send_to_splunk(event_data):\n    \"\"\"Send event to Splunk HEC\"\"\"\n    payload = {\n        \"time\": int(datetime.now().timestamp()),\n        \"sourcetype\": \"databricks:audit\",\n        \"source\": \"databricks_api\",\n        \"event\": event_data\n    }\n\n    headers = {\n        \"Authorization\": f\"Splunk {SPLUNK_HEC_TOKEN}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.post(SPLUNK_HEC_URL, headers=headers, json=payload, verify=True)\n    return response.status_code == 200\n\n# Query audit logs from system tables and forward to Splunk\naudit_logs = spark.sql(\"\"\"\n    SELECT *\n    FROM system.access.audit\n    WHERE event_time >= CURRENT_TIMESTAMP - INTERVAL 1 HOUR\n\"\"\").collect()\n\nfor log in audit_logs:\n    send_to_splunk(log.asDict())</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Key Metrics to Monitor\n\nBuild dashboards in your observability platform for these critical metrics:\n\n| Metric Category | Specific Metrics | Alert Thresholds |\n|-----------------|------------------|------------------|\n| **Cluster Health** | Cluster start failures, node evictions, OOM errors | >5% failure rate |\n| **Job Reliability** | Job success rate, retry counts, execution time P95/P99 | <95% success rate |\n| **Query Performance** | Query duration P95/P99, queue time, error rate | >10s P95 latency |\n| **Cost** | Daily DBU spend, cost per query, cost per job | >20% day-over-day increase |\n| **Concurrency** | Active queries, queued queries, warehouse utilization | Queue depth >10 |\n| **Data Quality** | DLT expectations pass rate, row counts, schema changes | <99% pass rate |\n| **Security** | Failed login attempts, privilege escalations, data exfiltration patterns | >10 failures/hour |\n\n<div style=\"background-color: #E3F2FD; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 16px 0;\">\n<strong>üí° Recommendation: Use System Tables for Metrics</strong><br/><br/>\nDatabricks System Tables (<code>system.query.history</code>, <code>system.access.audit</code>, <code>system.billing.usage</code>) provide SQL-queryable access to all operational metrics. This enables:\n<ul>\n<li>Custom dashboards in Databricks SQL or your BI tool</li>\n<li>Scheduled queries that export metrics to external systems</li>\n<li>Alert queries that trigger notifications via webhooks</li>\n<li>Historical trend analysis without log retention limits</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Alert Policies and Incident Response\n\n### Databricks Alert Framework\n\nAlerts transform monitoring data into actionable notifications. Databricks provides native alerting capabilities plus integration with external systems.\n\n<br />\n<div class=\"mermaid\">\nflowchart TB\n    subgraph \"Alert Sources\"\n        A[SQL Query Alert<br/>Scheduled Query]\n        B[Job Failure Alert<br/>Workflow Notification]\n        C[DLT Expectations<br/>Data Quality]\n        D[Custom Metrics<br/>System Tables]\n    end\n\n    subgraph \"Alert Routing\"\n        E[Alert Destination]\n    end\n\n    subgraph \"Notification Channels\"\n        F[Email]\n        G[Slack/Teams]\n        H[PagerDuty]\n        I[Webhook<br/>Custom Integration]\n        J[ITSM<br/>ServiceNow/Jira]\n    end\n\n    A --> E\n    B --> E\n    C --> E\n    D --> E\n\n    E --> F\n    E --> G\n    E --> H\n    E --> I\n    E --> J\n\n    style A fill:#E3F2FD\n    style B fill:#E3F2FD\n    style C fill:#FFF3E0\n    style D fill:#FFF3E0\n    style H fill:#FFEBEE\n</div>\n<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Alert Types and Configuration\n\n**1. SQL Query Alerts** (for operational metrics)\n\n<div class=\"code-block\" data-language=\"sql\">-- Alert query: Detect failed jobs in the past hour\nSELECT\n  job_id,\n  job_name,\n  run_id,\n  start_time,\n  end_time,\n  result_state,\n  error_message\nFROM\n  system.lakeflow.job_run_timeline\nWHERE\n  start_time >= CURRENT_TIMESTAMP - INTERVAL 1 HOUR\n  AND result_state = 'FAILED'\n  AND job_name NOT LIKE '%test%'  -- Exclude test jobs\nORDER BY\n  start_time DESC;\n\n-- Configure this query to run every 15 minutes\n-- Alert condition: Row count > 0\n-- Notification: Email + Slack</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div class=\"code-block\" data-language=\"sql\">-- Alert query: Detect cost anomalies\nSELECT\n  usage_date,\n  SUM(usage_quantity) AS total_dbus,\n  AVG(SUM(usage_quantity)) OVER (\n    ORDER BY usage_date\n    ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING\n  ) AS avg_30day_dbus,\n  STDDEV(SUM(usage_quantity)) OVER (\n    ORDER BY usage_date\n    ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING\n  ) AS stddev_30day_dbus\nFROM\n  system.billing.usage\nWHERE\n  usage_date >= CURRENT_DATE - INTERVAL 60 DAYS\nGROUP BY\n  usage_date\nHAVING\n  total_dbus > avg_30day_dbus + (2 * stddev_30day_dbus)  -- 2 sigma threshold\nORDER BY\n  usage_date DESC\nLIMIT 1;\n\n-- Alert condition: Row count > 0 (anomaly detected)\n-- Notification: Email finance team + platform team</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**2. Job Failure Notifications**\n\nConfigure notifications directly in Lakeflow Jobs (Workflows):\n\n<div class=\"code-block\" data-language=\"python\"># Using Databricks SDK to create job with notifications\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.jobs import (\n    JobEmailNotifications, JobNotificationSettings,\n    JobSettings, Task, NotebookTask, Source\n)\n\nw = WorkspaceClient()\n\njob = w.jobs.create(\n    name=\"critical_etl_pipeline\",\n    tasks=[\n        Task(\n            task_key=\"ingest_data\",\n            notebook_task=NotebookTask(\n                notebook_path=\"/Pipelines/Ingest\",\n                source=Source.WORKSPACE\n            ),\n            email_notifications=JobEmailNotifications(\n                on_failure=[\"data-eng-oncall@company.com\"],\n                on_success=[\"data-eng-team@company.com\"]\n            )\n        )\n    ],\n    email_notifications=JobEmailNotifications(\n        on_failure=[\"data-platform-oncall@company.com\", \"manager@company.com\"],\n        no_alert_for_skipped_runs=True\n    ),\n    webhook_notifications={\n        \"on_failure\": [{\n            \"id\": \"pagerduty_webhook_id\"\n        }]\n    },\n    notification_settings=JobNotificationSettings(\n        no_alert_for_canceled_runs=True,\n        no_alert_for_skipped_runs=True\n    )\n)</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**3. DLT Data Quality Alerts**\n\nSpark Declarative Pipelines (DLT) can alert on expectation failures:\n\n<div class=\"code-block\" data-language=\"python\"># In a DLT pipeline notebook\nimport dlt\nfrom pyspark.sql.functions import col\n\n@dlt.table(\n    name=\"clean_customer_data\",\n    comment=\"Customer data with quality expectations\"\n)\n@dlt.expect_all_or_fail({\n    \"valid_email\": \"email IS NOT NULL AND email LIKE '%@%'\",\n    \"valid_created_date\": \"created_date >= '2020-01-01'\"\n})\n@dlt.expect_or_drop(\"positive_customer_id\", \"customer_id > 0\")\ndef clean_customers():\n    return spark.read.table(\"raw_customers\")\n\n# Configure DLT pipeline settings to send alerts on expectation failures\n# UI: Settings > Email Notifications > \"On Expectation Failures\"</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Incident Response Workflow\n\nA mature incident response process ensures alerts lead to resolution, not just notifications.\n\n| Phase | Activities | Responsible Party | Tools/Systems |\n|-------|-----------|-------------------|---------------|\n| **Detection** | Alert fires based on metric threshold or failure | Automated monitoring | Databricks Alerts, CloudWatch, PagerDuty |\n| **Triage** | On-call engineer assesses severity and impact | Platform SRE / Data Engineer | System tables, job logs, query history |\n| **Communication** | Notify stakeholders; create incident ticket | On-call engineer | Slack, ServiceNow, Jira, StatusPage |\n| **Investigation** | Root cause analysis using logs and lineage | Data Engineering team | Databricks workspace, Unity Catalog lineage |\n| **Mitigation** | Apply temporary fix or rollback change | Data Engineering team | Git revert, job disable, cluster restart |\n| **Resolution** | Implement permanent fix; verify recovery | Data Engineering team | Code changes, infrastructure updates |\n| **Postmortem** | Document RCA, action items, prevention measures | Team lead | Wiki, postmortem doc, retrospective meeting |\n\n<div style=\"background-color: #FFF3E0; border-left: 4px solid #FF9800; padding: 12px 16px; margin: 16px 0;\">\n<strong>‚öôÔ∏è Best Practice: Runbooks for Common Incidents</strong><br/><br/>\nDocument standard operating procedures for frequent incident types:\n<ul>\n<li><strong>Job Failure</strong>: Check recent code changes, verify input data availability, review cluster logs for OOM/timeout errors</li>\n<li><strong>Query Performance Degradation</strong>: Check for table bloat, analyze query plans, verify warehouse capacity</li>\n<li><strong>Cost Spike</strong>: Identify runaway jobs via <code>system.billing.usage</code>, terminate idle clusters, review recent deployments</li>\n<li><strong>Data Quality Issue</strong>: Trace lineage to source, check DLT expectations, notify upstream data providers</li>\n<li><strong>Access Denied</strong>: Verify Unity Catalog permissions, check recent REVOKE statements, confirm identity provider sync</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Alert Destination Configuration\n\n**Slack Integration Example:**\n\n<div class=\"code-block\" data-language=\"python\"># Create a webhook destination for Slack notifications\nfrom databricks.sdk import WorkspaceClient\nfrom databricks.sdk.service.sql import AlertDestination\n\nw = WorkspaceClient()\n\n# Create Slack webhook destination\nslack_dest = w.alert_destinations.create(\n    name=\"data_eng_slack_channel\",\n    config={\n        \"url\": \"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\"\n    },\n    destination_type=\"WEBHOOK\"\n)\n\nprint(f\"Slack destination created: {slack_dest.id}\")</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**PagerDuty Integration Example:**\n\n<div class=\"code-block\" data-language=\"python\"># Create PagerDuty webhook destination\npagerduty_dest = w.alert_destinations.create(\n    name=\"oncall_pagerduty\",\n    config={\n        \"url\": \"https://events.pagerduty.com/v2/enqueue\",\n        \"headers\": {\n            \"Authorization\": \"Token token=YOUR_PAGERDUTY_API_KEY\"\n        },\n        \"template\": {\n            \"routing_key\": \"YOUR_PAGERDUTY_INTEGRATION_KEY\",\n            \"event_action\": \"trigger\",\n            \"payload\": {\n                \"summary\": \"Databricks Alert: {{ ALERT_NAME }}\",\n                \"severity\": \"error\",\n                \"source\": \"databricks\",\n                \"custom_details\": {\n                    \"alert_url\": \"{{ ALERT_URL }}\",\n                    \"query_url\": \"{{ QUERY_URL }}\"\n                }\n            }\n        }\n    },\n    destination_type=\"WEBHOOK\"\n)</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Infrastructure as Code (IaC)\n\n### The Challenge: Platform Drift and Change Management\n\nPost-migration, your Databricks platform will evolve: new workspaces, catalog changes, cluster policy updates, job definitions, and permission grants. Without IaC, these changes become:\n- **Undocumented**: No record of who changed what and why\n- **Unrepeatable**: Manual UI changes can't be replicated across environments\n- **Unauditable**: Compliance teams can't verify configurations\n- **Fragile**: No way to rollback breaking changes\n\nIaC solves this by treating infrastructure configuration as versioned code subject to the same rigor as application code: peer review, CI/CD, automated testing, and rollback capabilities.\n\n<br />\n<div class=\"mermaid\">\ngraph LR\n    subgraph \"Development Flow\"\n        A[Local Config<br/>Changes] --> B[Git Commit]\n        B --> C[Pull Request]\n        C --> D[Peer Review]\n        D --> E[CI/CD Pipeline]\n        E --> F{Validation<br/>Pass?}\n        F -->|No| G[Fix Issues]\n        F -->|Yes| H[Deploy to Dev]\n        G --> A\n        H --> I[Promote to Prod]\n    end\n\n    subgraph \"Databricks Platform\"\n        J[Workspaces]\n        K[Clusters]\n        L[Jobs]\n        M[Unity Catalog]\n        N[Permissions]\n    end\n\n    I --> J\n    I --> K\n    I --> L\n    I --> M\n    I --> N\n\n    style A fill:#E3F2FD\n    style C fill:#FFF3E0\n    style E fill:#F3E5F5\n    style I fill:#E8F5E9\n</div>\n<script type=\"module\"> import mermaid from \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs\"; mermaid.initialize({ startOnLoad: true, theme: \"default\" }); </script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### IaC Options for Databricks\n\n| Tool | Strengths | Best For | Learning Curve |\n|------|-----------|----------|----------------|\n| **Terraform** | Mature, multi-cloud, large ecosystem, declarative | Cross-platform infrastructure; teams already using Terraform | Medium |\n| **Databricks Asset Bundles (DABs)** | Native Databricks tooling, Git-integrated, YAML-based | Application deployment (jobs, pipelines, notebooks); developer workflows | Low |\n| **Databricks Terraform Provider** | Official Databricks provider for Terraform; comprehensive resource coverage | Account-level config, workspaces, Unity Catalog, clusters, jobs | Medium |\n| **Pulumi** | Multi-language (Python, TypeScript, Go), strong typing | Teams preferring code over YAML/HCL; complex logic in IaC | Medium-High |\n| **Ansible** | Agentless, procedural, existing playbooks | Organizations standardized on Ansible; configuration management | Medium |\n\n<div style=\"background-color: #E3F2FD; border-left: 4px solid #2196F3; padding: 12px 16px; margin: 16px 0;\">\n<strong>üí° Recommendation: Hybrid Approach</strong><br/><br/>\n<ul>\n<li><strong>Terraform</strong>: Manage account-level resources (workspaces, metastores, storage credentials, network configs)</li>\n<li><strong>Databricks Asset Bundles</strong>: Deploy application-layer resources (jobs, DLT pipelines, notebooks, dashboards)</li>\n</ul>\nThis separation aligns with ownership: platform team owns infrastructure (Terraform), data teams own applications (DABs).\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Terraform for Databricks: Core Patterns\n\n**Provider Configuration:**\n\n<div class=\"code-block\" data-language=\"hcl\"># providers.tf\nterraform {\n  required_providers {\n    databricks = {\n      source  = \"databricks/databricks\"\n      version = \"~> 1.35\"\n    }\n  }\n}\n\n# Authenticate using Service Principal (recommended for CI/CD)\nprovider \"databricks\" {\n  host       = var.databricks_host\n  client_id  = var.databricks_client_id\n  client_secret = var.databricks_client_secret\n}\n\n# Alternative: Account-level provider for managing workspaces\nprovider \"databricks\" {\n  alias      = \"account\"\n  host       = \"https://accounts.cloud.databricks.com\"\n  account_id = var.databricks_account_id\n  client_id  = var.databricks_client_id\n  client_secret = var.databricks_client_secret\n}</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-hcl.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Managing SQL Warehouses:**\n\n<div class=\"code-block\" data-language=\"hcl\"># sql_warehouses.tf\nresource \"databricks_sql_endpoint\" \"analytics_prod\" {\n  name             = \"analytics_warehouse_prod\"\n  cluster_size     = \"Medium\"\n  max_num_clusters = 5\n  auto_stop_mins   = 15\n\n  tags {\n    custom_tags = {\n      cost_center = \"analytics\"\n      team        = \"bi_reporting\"\n      environment = \"prod\"\n      managed_by  = \"terraform\"\n    }\n  }\n\n  channel {\n    name = \"CHANNEL_NAME_CURRENT\"  # Use current channel for latest features\n  }\n}\n\nresource \"databricks_sql_endpoint\" \"data_science_dev\" {\n  name             = \"data_science_warehouse_dev\"\n  cluster_size     = \"Small\"\n  max_num_clusters = 2\n  auto_stop_mins   = 10\n\n  enable_serverless_compute = true\n\n  tags {\n    custom_tags = {\n      cost_center = \"data_science\"\n      team        = \"ml_platform\"\n      environment = \"dev\"\n      managed_by  = \"terraform\"\n    }\n  }\n}</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-hcl.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Managing Unity Catalog Resources:**\n\n<div class=\"code-block\" data-language=\"hcl\"># unity_catalog.tf\nresource \"databricks_catalog\" \"analytics\" {\n  name         = \"analytics_prod\"\n  comment      = \"Production analytics catalog - managed by Terraform\"\n  owner        = \"data_platform_team\"\n  isolation_mode = \"OPEN\"\n\n  properties = {\n    managed_by   = \"terraform\"\n    cost_center  = \"analytics\"\n    created_date = timestamp()\n  }\n}\n\nresource \"databricks_schema\" \"sales\" {\n  catalog_name = databricks_catalog.analytics.name\n  name         = \"sales\"\n  comment      = \"Sales domain data\"\n  owner        = \"sales_analytics_team\"\n}\n\nresource \"databricks_grants\" \"sales_schema_access\" {\n  schema = \"${databricks_catalog.analytics.name}.${databricks_schema.sales.name}\"\n\n  grant {\n    principal  = \"sales_analysts\"\n    privileges = [\"SELECT\", \"USE_SCHEMA\", \"USE_CATALOG\"]\n  }\n\n  grant {\n    principal  = \"sales_engineers\"\n    privileges = [\"SELECT\", \"MODIFY\", \"USE_SCHEMA\", \"USE_CATALOG\"]\n  }\n}</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-hcl.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Managing Cluster Policies:**\n\n<div class=\"code-block\" data-language=\"hcl\"># cluster_policies.tf\nresource \"databricks_cluster_policy\" \"cost_optimized\" {\n  name = \"cost_optimized_policy\"\n\n  definition = jsonencode({\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"14.3.x-scala2.12\"\n    },\n    \"node_type_id\": {\n      \"type\": \"allowlist\",\n      \"values\": [\"i3.xlarge\", \"i3.2xlarge\", \"m5d.large\"]\n    },\n    \"autotermination_minutes\": {\n      \"type\": \"range\",\n      \"minValue\": 10,\n      \"maxValue\": 120,\n      \"defaultValue\": 30\n    },\n    \"custom_tags.cost_center\": {\n      \"type\": \"fixed\",\n      \"value\": \"{{user_email.split('@')[1].split('.')[0]}}\"  # Infer from user email\n    },\n    \"custom_tags.managed_by\": {\n      \"type\": \"fixed\",\n      \"value\": \"terraform\"\n    },\n    \"aws_attributes.spot_bid_price_percent\": {\n      \"type\": \"fixed\",\n      \"value\": 100  # Use spot instances\n    }\n  })\n}\n\nresource \"databricks_permissions\" \"cost_optimized_usage\" {\n  cluster_policy_id = databricks_cluster_policy.cost_optimized.id\n\n  access_control {\n    group_name       = \"data_engineers\"\n    permission_level = \"CAN_USE\"\n  }\n}</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-hcl.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### Databricks Asset Bundles: Application Deployment\n\nDatabricks Asset Bundles (DABs) provide a Git-native way to deploy jobs, pipelines, and notebooks as versioned applications.\n\n**Bundle Structure:**\n\n<div class=\"code-block\" data-language=\"yaml\"># databricks.yml (root of your Git repository)\nbundle:\n  name: sales_analytics_pipeline\n\ninclude:\n  - resources/*.yml\n\ntargets:\n  dev:\n    mode: development\n    workspace:\n      host: https://dev-workspace.cloud.databricks.com\n    variables:\n      catalog: dev_catalog\n      warehouse_id: abc123dev\n\n  prod:\n    mode: production\n    workspace:\n      host: https://prod-workspace.cloud.databricks.com\n    variables:\n      catalog: prod_catalog\n      warehouse_id: xyz789prod\n    # Require approval for production deployments\n    run_as:\n      service_principal_name: \"prod-deployment-sp\"</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Job Definition in Bundle:**\n\n<div class=\"code-block\" data-language=\"yaml\"># resources/sales_etl_job.yml\nresources:\n  jobs:\n    sales_daily_aggregation:\n      name: \"Sales Daily Aggregation - ${bundle.target}\"\n\n      tasks:\n        - task_key: ingest_raw_sales\n          notebook_task:\n            notebook_path: ../notebooks/ingest_sales.py\n            source: GIT\n          new_cluster:\n            spark_version: 14.3.x-scala2.12\n            node_type_id: i3.xlarge\n            num_workers: 4\n            custom_tags:\n              cost_center: \"sales\"\n              team: \"analytics\"\n              environment: \"${bundle.target}\"\n\n        - task_key: transform_sales\n          depends_on:\n            - task_key: ingest_raw_sales\n          notebook_task:\n            notebook_path: ../notebooks/transform_sales.py\n            source: GIT\n          existing_cluster_id: \"${var.shared_cluster_id}\"\n\n      schedule:\n        quartz_cron_expression: \"0 0 2 * * ?\"  # 2 AM daily\n        timezone_id: \"America/New_York\"\n        pause_status: PAUSED  # Start paused; enable after validation\n\n      email_notifications:\n        on_failure:\n          - \"sales-analytics-oncall@company.com\"\n        on_success:\n          - \"sales-analytics-team@company.com\"\n\n      tags:\n        cost_center: \"sales\"\n        managed_by: \"databricks_asset_bundle\"</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**Deploying with DABs:**\n\n<div class=\"code-block\" data-language=\"bash\"># Install Databricks CLI with Asset Bundles support\ncurl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh\n\n# Authenticate to workspace\ndatabricks configure\n\n# Validate bundle configuration\ndatabricks bundle validate -t dev\n\n# Deploy to development\ndatabricks bundle deploy -t dev\n\n# Run the deployed job\ndatabricks bundle run sales_daily_aggregation -t dev\n\n# Promote to production (after validation)\ndatabricks bundle deploy -t prod</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### CI/CD Pipeline Integration\n\n**GitHub Actions Example for Terraform:**\n\n<div class=\"code-block\" data-language=\"yaml\"># .github/workflows/terraform-databricks.yml\nname: Terraform - Databricks Infrastructure\n\non:\n  pull_request:\n    branches: [main]\n    paths:\n      - 'terraform/**'\n  push:\n    branches: [main]\n    paths:\n      - 'terraform/**'\n\njobs:\n  terraform:\n    runs-on: ubuntu-latest\n\n    env:\n      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n      DATABRICKS_CLIENT_ID: ${{ secrets.DATABRICKS_CLIENT_ID }}\n      DATABRICKS_CLIENT_SECRET: ${{ secrets.DATABRICKS_CLIENT_SECRET }}\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.6.0\n\n      - name: Terraform Init\n        working-directory: ./terraform\n        run: terraform init\n\n      - name: Terraform Validate\n        working-directory: ./terraform\n        run: terraform validate\n\n      - name: Terraform Plan\n        working-directory: ./terraform\n        run: terraform plan -out=tfplan\n\n      - name: Terraform Apply\n        if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n        working-directory: ./terraform\n        run: terraform apply -auto-approve tfplan</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "**GitHub Actions Example for Asset Bundles:**\n\n<div class=\"code-block\" data-language=\"yaml\"># .github/workflows/databricks-bundle-deploy.yml\nname: Deploy Databricks Asset Bundle\n\non:\n  push:\n    branches: [main, develop]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install Databricks CLI\n        run: |\n          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh\n          echo \"$HOME/.databricks/bin\" >> $GITHUB_PATH\n\n      - name: Validate Bundle\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n        run: |\n          TARGET=${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}\n          databricks bundle validate -t $TARGET\n\n      - name: Deploy to Dev\n        if: github.ref == 'refs/heads/develop'\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_DEV_HOST }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEV_TOKEN }}\n        run: databricks bundle deploy -t dev\n\n      - name: Deploy to Prod\n        if: github.ref == 'refs/heads/main'\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_PROD_HOST }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_PROD_TOKEN }}\n        run: databricks bundle deploy -t prod</div>\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css\">\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-yaml.min.js\"></script>\n<script>\ndocument.addEventListener('DOMContentLoaded', (event) => {\n  document.querySelectorAll('.code-block').forEach((block) => {\n    const code = block.textContent;\n    const language = block.getAttribute('data-language') || 'python';\n    block.innerHTML = `<pre><code class=\"language-${language}\">${code.trim()}</code></pre>`;\n  });\n  Prism.highlightAll();\n});\n</script>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "### IaC Best Practices\n\n| Practice | Why It Matters | Implementation |\n|----------|----------------|----------------|\n| **State Management** | Terraform state must be shared and locked | Use remote backend (S3 + DynamoDB, Azure Storage, Terraform Cloud) |\n| **Environment Separation** | Dev changes shouldn't impact prod | Separate state files per environment; use workspaces or directories |\n| **Secret Management** | Never commit credentials to Git | Use GitHub Secrets, AWS Secrets Manager, Azure Key Vault, HashiCorp Vault |\n| **Drift Detection** | Detect manual changes outside IaC | Run `terraform plan` daily; alert on drift; use Terraform Cloud for automated detection |\n| **Modular Design** | Reusable, composable infrastructure | Create Terraform modules for common patterns (warehouse, cluster policy, catalog) |\n| **Code Review** | Prevent misconfigurations | Require PR approval for all infrastructure changes; use automated validation tools |\n| **Rollback Strategy** | Quick recovery from bad changes | Tag releases; maintain previous Terraform state; test rollback procedures |\n| **Documentation** | Knowledge transfer and onboarding | Document module usage, variable meanings, and deployment procedures |\n\n<div style=\"background-color: #FFEBEE; border-left: 4px solid #F44336; padding: 12px 16px; margin: 16px 0;\">\n<strong>‚ö†Ô∏è Common IaC Pitfalls</strong><br/><br/>\n<ul>\n<li><strong>Destructive Changes</strong>: Some Terraform changes trigger resource replacement (deletion + recreation). Always review plan output for <code>-/+</code> (replace) operations.</li>\n<li><strong>State File Loss</strong>: Losing Terraform state means losing track of managed resources. Always use remote state with versioning.</li>\n<li><strong>Hardcoded Values</strong>: Hardcoding workspace IDs, cluster IDs, or credentials makes code brittle. Use variables and data sources.</li>\n<li><strong>Incomplete Migration</strong>: Mixing manual changes with IaC creates drift and confusion. Commit to full IaC adoption or define boundaries clearly.</li>\n</ul>\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Summary: Operational Maturity Checklist\n\nAssess your post-migration operational readiness:\n\n### Compute Optimization\n- [ ] Serverless SQL Warehouses configured with appropriate size and max clusters\n- [ ] Warehouse-level tagging for cost attribution\n- [ ] Monitoring in place for queue times and concurrency\n- [ ] Cluster policies enforce cost controls and tagging requirements\n\n### Cost Management\n- [ ] Billing tags defined and enforced across all compute resources\n- [ ] `system.billing.usage` dashboards built for cost visibility\n- [ ] Budget alerts configured for teams and cost centers\n- [ ] Untagged usage < 5% of total spend\n- [ ] Monthly cost review process established\n\n### Observability\n- [ ] Log delivery configured (CloudWatch/Azure Monitor/Splunk)\n- [ ] System table queries built for operational metrics\n- [ ] Dashboards created for job health, query performance, and costs\n- [ ] Integration with enterprise monitoring platform complete\n\n### Alerting & Incident Response\n- [ ] SQL query alerts configured for critical metrics\n- [ ] Job failure notifications routed to appropriate teams\n- [ ] PagerDuty/on-call integration for production incidents\n- [ ] Incident response runbooks documented\n- [ ] Postmortem process defined\n\n### Infrastructure as Code\n- [ ] Terraform or DABs adopted for infrastructure management\n- [ ] CI/CD pipeline configured for automated deployments\n- [ ] Remote state backend configured with locking\n- [ ] Drift detection process in place\n- [ ] Dev/staging/prod environment separation established\n\n<div style=\"background-color: #E8F5E9; border-left: 4px solid #4CAF50; padding: 12px 16px; margin: 16px 0;\">\n<strong>‚úÖ Next Steps</strong><br/><br/>\nWith operational practices in place, the platform is ready for broader adoption. The next module focuses on consumer integration - enabling downstream users and applications to leverage the migrated platform.\n</div>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "## Related Resources\n\n- [Databricks SQL Warehouse Documentation](https://docs.databricks.com/sql/admin/sql-endpoints.html)\n- [System Tables Reference](https://docs.databricks.com/admin/system-tables/index.html)\n- [Databricks Terraform Provider](https://registry.terraform.io/providers/databricks/databricks/latest/docs)\n- [Databricks Asset Bundles Guide](https://docs.databricks.com/dev-tools/bundles/index.html)\n- [Unity Catalog Best Practices](https://docs.databricks.com/data-governance/unity-catalog/best-practices.html)\n\n---\n\n**Next Lesson:** [5.2 - Consumer Integration]($./5.2 - Consumer Integration)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "<div style=\"color: #FF3621; font-weight: bold; font-size: 2em; margin-bottom: 12px;\">COURSE DEVELOPER (remove before publishing)</div>\n\n### Customization Notes\n\n**Platform-Specific Additions:**\n\nWhen customizing for a specific source platform (Snowflake, Redshift, BigQuery, Synapse, etc.), add:\n\n1. **Compute Mapping**: Document how {SOURCE_PLATFORM} compute resources (warehouses, clusters, serverless) map to Databricks equivalents and typical sizing conversions\n\n2. **Cost Comparison**: Provide specific examples of DBU-based pricing vs. source platform pricing models; include sample cost calculations for common workload types\n\n3. **Observability Tools**: If migrating from a platform with specific monitoring tools (e.g., Snowflake Resource Monitors, Redshift Performance Insights), document equivalent capabilities in Databricks\n\n4. **IaC Patterns**: If the source platform uses specific IaC tools (e.g., SnowSQL scripts, CloudFormation for Redshift), provide migration patterns to Terraform/DABs\n\n5. **Tagging Standards**: If the organization has existing tagging taxonomies from cloud providers or source platforms, map them to Databricks custom tags\n\n6. **Alert Templates**: Provide platform-specific alert query templates based on common operational patterns from the source system\n\n**Integration Examples:**\n\nAdd code examples for:\n- Source platform-specific log forwarding patterns\n- ITSM integrations (ServiceNow, Jira) relevant to the organization\n- Cost allocation to existing finance systems (SAP, Oracle Financials)\n- Compliance reporting specific to industry (HIPAA, SOX, GDPR)\n\n**Hands-On Labs:**\n\nConsider adding:\n- Lab 1: Configure a Serverless SQL Warehouse and monitor performance\n- Lab 2: Build a cost attribution dashboard using `system.billing.usage`\n- Lab 3: Set up log delivery and create alerts\n- Lab 4: Deploy infrastructure using Terraform or Asset Bundles"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cell-46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "",
     "isMarkdownSandbox": true
    }
   },
   "source": "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": {},
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}